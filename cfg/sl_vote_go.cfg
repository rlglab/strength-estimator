# Program
program_seed=0 # assign a program seed
program_auto_seed=true # true for assigning a random seed automatically
program_quiet=false # true for silencing the error message

# Actor
actor_num_simulation=800 # simulation number of MCTS
actor_mcts_puct_base=19652 # hyperparameter for puct_bias in the PUCT formula of MCTS, determining the level of exploration
actor_mcts_puct_init=1.25 # hyperparameter for puct_bias in the PUCT formula of MCTS
actor_mcts_reward_discount=1 # discount factor for calculating Q values
actor_mcts_value_rescale=false # true for games whose rewards are not bounded in [-1, 1], e.g., Atari games
actor_mcts_think_batch_size=1 # the MCTS selection batch size; only works when running console
actor_mcts_think_time_limit=0 # the MCTS time limit in seconds, 0 represents disabling time limit (only uses actor_num_simulation); only works when running console
actor_select_action_by_count=false # true for selecting the action by the maximum MCTS count; should not be true together with actor_select_action_by_softmax_count
actor_select_action_by_softmax_count=true # true for selecting the action by the propotion of MCTS count; should not be true together with actor_select_action_by_count
actor_select_action_softmax_temperature=1 # the softmax temperature when using actor_select_action_by_softmax_count
# true for decaying the temperature based on training iteration; set 1, 0.5, and 0.25 for 0%-50%, 50%-75%, and 75%-100% of total iterations, respectively
actor_select_action_softmax_temperature_decay=false
actor_use_random_rotation_features=true # true for randomly rotating input features; only supports in alphazero
actor_use_dirichlet_noise=true # true for adding dirchlet noise to the policy
actor_dirichlet_noise_alpha=0.03 # hyperparameter for dirchlet noise, usually (1 / sqrt(number of actions))
actor_dirichlet_noise_epsilon=0.25 # hyperparameter for dirchlet noise
actor_use_gumbel=false # true for enabling Gumbel Zero
actor_use_gumbel_noise=false # true for adding Gumbel noise to the policy
actor_gumbel_sample_size=16 # hyperparameter for Gumbel Zero; the number of sampled actions
actor_gumbel_sigma_visit_c=50 # hyperparameter for the monotonically increasing transformation sigma in Gumbel Zero
actor_gumbel_sigma_scale_c=1 # hyperparameter for the monotonically increasing transformation sigma in Gumbel Zero
actor_resign_threshold=-0.9 # the threshold determining when to resign in the actor
actor_select_action_by_ssa=false
actor_select_action_by_bt=false

# Zero
zero_num_threads=4 # the number of threads that the zero server uses for zero training
zero_num_parallel_games=32 # the number of games to be run in parallel for zero training
zero_server_port=9999 # the port number to host the server; workers should connect to this port number
zero_training_directory= # the output directory name for storing training results
zero_num_games_per_iteration=2000 # the nunmber of games to play in each iteration
zero_start_iteration=0 # the first iteration of training; usually 1 unless continuing with previous training
zero_end_iteration=100 # the last iteration of training
zero_replay_buffer=20 # hyperparameter for replay buffer; replay buffer stores (zero_replay_buffer x zero_num_games_per_iteration) games/sequences
zero_disable_resign_ratio=0.1 # the probability to keep playing when the winrate is below actor_resign_threshold
zero_actor_intermediate_sequence_length=0 # the max sequence length when running self-play; usually 0 (unlimited) for board games, 200 for atari games
zero_actor_ignored_command=reset_actors # the commands to ignore by the actor; format: command1 command2 ...
zero_server_accept_different_model_games=true # true for accepting self-play games generated by out-of-date model

# Learner
learner_use_per=false # true for enabling Prioritized Experience Replay
learner_per_alpha=1 # hyperparameter for PER that controlls the probability of sampling transition
learner_per_init_beta=1 # hyperparameter for PER that sets the initial beta value of linearly annealing
learner_per_beta_anneal=true # hyperparameter for PER that enables linearly anneal beta based on current training iteration
learner_training_step=130000 # the number of training steps for updating the model in each iteration
learner_training_display_step=100 # the training step interval to display training information
learner_batch_size=1024 # the batch size for training
learner_muzero_unrolling_step=5 # the number of steps to unroll for muzero training
learner_n_step_return=0 # the number of steps to calculate the n-step value; usually 0 for board games, 10 for atari games
learner_learning_rate=0.01 # hyperparameter for learning rate
learner_momentum=0.9 # hyperparameter for momentum
learner_weight_decay=0.0001 # hyperparameter for weight decay
learner_value_loss_scale=1 # hyperparameter for scaling of the value loss
learner_num_thread=8 # the number of threads for training

# Network
nn_file_name=./model/sl_go.pt # the file name of model weights
nn_num_blocks=20 # hyperparameter for the model; the number of the residual blocks
nn_num_hidden_channels=256 # hyperparameter for the model; the size of the hidden channels in residual blocks
nn_num_value_hidden_channels=256 # hyperparameter for the model; the size of the hidden channels in the value network
nn_type_name=rank # the type of training algorithm and network: alphazero/muzero

# Environment
env_board_size=19 # the size of board
env_go_komi=7.5 # the komi in Go
env_go_ko_rule=positional # the ko rules in Go: positional (only consider stones), situational (consider stones and the turn)

# Strength
nn_rank_size=11
bt_num_batch_size=32 # batch size in bt training = bt_num_batch_size * bt_num_rank_per_batch * bt_num_position_per_rank
bt_num_rank_per_batch=11 # batch size in bt training = bt_num_batch_size * bt_num_rank_per_batch * bt_num_position_per_rank
bt_num_position_per_rank=7 # batch size in bt training = bt_num_batch_size * bt_num_rank_per_batch * bt_num_position_per_rank
bt_use_weight=false
bt_use_same_game_per_rank=true
bt_add_non_people=false # when training bt, add the non-people move to be the lowest rank
training_sgf_dir=training_sgf_go
testing_sgf_dir=query_sgf_go
candidate_sgf_dir=candidate_sgf_go
evaluator_mode=game_prediction # game_prediction/move_prediction
rank_mode=max_num # max_prob/max_num
accuracy_mode=+/-0 # +/-0,+/-1,+1,-1
select_move=all_moves # all_moves/first_50_moves/last_50_moves/one_move_per_game
s_weight=2.0 # weight for puct value_s

